---
layout: distill
title: >-
  Content Promotion as a Strategic Game: How to Design Agentic Publishers for the Evolving Search Ecosystem in the GenAI Era?
description: With the rise of LLMs, publishers now operate in a dual world where traditional search and chat-like systems coexist. We propose a unified, game-theoretic view of this environment and highlight different tools, such as Multi-Agent Reinforcement Learning, that support the development of competitive content-optimization agents.
date: 2026-04-27
future: true
htmlwidgets: true

# Anonymize for the review process!
authors:
  - name: Anonymous

# Link to the bibliography file
bibliography: 2026-04-27-content-promotion-agent-design.bib

# Table of Contents
toc:
  - name: LLMs Reshape Both Sides of the Search Ecosystem
  - name: Publisher Incentives Across Coexisting Ecosystems
    subsections:
        - name: Publishers' Incentives in the Traditional Setting
        - name : Publishers' Incentives in the Chat-Like Settings
  - name: Leveraging LLMs for Content Optimization
  - name: Content Promotion as a Game
  - name: Agent Design Aspects for Traditional Search and Chat-like Systems
  - name: Frameworks
    subsections:
        - name: Multi-Agent Reinforcement Learning (MARL)
        - name: Synthetic Dataset Generation
        - name: Simulation Environment
        - name: Self-Play
        - name: Meta Games and Learning Equilibrium
  - name: Preliminary Results
    subsections:
        - name: Prompt-based Agent
        - name: RLRF-based Agent
        - name: LEMSS
        - name: Attacking RAG-based systems
  - name: Tools
  - name: Conclusion
---

## TL;DR
LLMs are transforming search from static ranked lists of results into interactive, chat-like
experiences. LLMs also reshape the way publishers (i.e., content creators) generate content:
they increasingly use LLMs and agentic tools to optimize rankings in traditional search and
to improve their content visibility in the novel chat-like systems. Importantly, publishers
engage in an iterative process: they observe how their content performs --- whether it is highly
ranked in the traditional search or visible in chat-like answers --- and then strategically modify
or regenerate their content in response to this feedback. As a result, the ecosystem
becomes a strategic game with incomplete information, evolving feedback, and repeated
interactions. In this strategic ecosystem, the publishers act as players striving to promote
their content, competing for rankings and visibility under mechanisms that are only partially
known to them. This blog outlines why game theory offers a natural framework to model
these settings and to design agents acting on behalf of publishers. We highlight several
frameworks such as Game theory and Multi-agent Reinforcement learning and show how
they contribute to model and design effective agents. Our aim is to provide principled lens for content promotion in the LLM era.

## LLMs Reshape Both Sides of the Search Ecosystem

Large language models have fundamentally reshaped the search experience <d-cite key=metzler_rethinking_2021></d-cite>. While users once typed short keyword queries and scanned a ranked list of links,
modern systems now offer chat-like interfaces that feel more like interacting with an
assistant than browsing a directory. These systems provide conversational question-answering (QA) capabilities by generating responses synthesized from multiple sources.
Search is no longer about “links” --- it is about responses. In the service of providing answers
to questions, paradigms such as Retrieval Augmented Generate (RAG) <d-cite key=gao_retrieval_augmented_2024></d-cite> have been developed. Similar paradigms were suggested for integrate commercial content
in the answer <d-cite key="feizi_online_2024, mordo_sponsored_2024"></d-cite>. The new chat-like
setting introduces new challenges, such as providing users with pointers to sources of
information, leading to techniques known as source attribution <d-cite key=bohnet_attributed_2023></d-cite>.
Interestingly, effective retrieval has become a necessity in this new setting: augmenting an
LLM with the ability to retrieve relevant information during answer generation or to retrieve
the most relevant source to attribute to related pieces of information.

But the transformation is not limited to the user side of the ecosystem.

LLMs are also reshaping the publisher side. In the traditional Web era, publishers focused
on keyword and hyperlink optimization, and content structure to influence ranking <d-cite key=gyongyi_web_2005></d-cite>. Today, publishers increasingly rely on LLMs:

- to generate or refine content;
- to adapt their content to the behavior of LLM-driven retrieval and QA systems;
- to strategically shape their content so that they are selected, quoted, or incorporated
into the ranked list of results or the generated answer.

The paradigm of helping publishers in improving their content visibility in generative engine
responses is often called Generative Engine Optimization (GEO) <d-cite key=aggarwal_geo_2024></d-cite>.

## Publisher Incentives Across Coexisting Ecosystems

The search ecosystem is not transitioning overnight --- it is evolving. Today, publishers
operate in a hybrid world where traditional search engines and LLM-driven chat interfaces coexist. Users still issue keyword queries and click ranked results, yet they also increasingly
engage with conversational systems that generate responses. As a result, publishers must
navigate two overlapping incentive structures at once.

### Publishers' Incentives in the Traditional Setting

In the traditional setting, publisher goals are well-defined and widely understood. Visibility
depends on ranking, which in turn hinges on factors, some of which are familiar:

- content relevance;
- content quality and structure;
- signals (e.g., PageRank <d-cite key=page_pagerank_1999></d-cite>).

Metrics such as impressions and CTR (Click Through Rate) offer interpretable objectives.

### Publishers' Incentives in the Chat-Like Settings

By contrast, the objectives in the chat-like settings are far less defined. Publishers know that
visibility is tied to whether the chat-like setting:

- retrieves their content;
- cites or incorporates it into the generated response;
- and positions it prominently in the generated response.

But unlike in traditional search, where there is a clear notion of a ranked results page, LLM-based systems provide no public ranking, and often little transparency about how sources
are selected for retrieval or attribution.

The publisher’s utility is therefore inherently vague: Is success defined by:

- citation count?
- contribution to the generated answer?
- similarity of the generated answer to the publisher’s content?
- some combination of the above?

The field has not yet converged on a clear success criterion, and the objective itself is still ill-defined, making strategic optimization an open problem.

## Leveraging LLMs for Content Optimization

Publishers --- or agents acting on behalf of them --- can leverage LLMs as powerful content
optimization tools (See Figures [1](#fig1) and [2](#fig2)). The publishers are incentivized to have effective
automatic generation to promote their content. However, any such agentic systems should
be carefully designed:

- address the tradeoff between “content promotion” (e.g., being selected in the search
results or being visible in the generated response) and “faithfulness” to the original
intent (e.g., not deviating from the content before optimization);
- address the uncertainty about the ranker or the content selection/generation
mechanism;
- address competing agents which aim to optimize contents of other stakeholders.

<div class="row mt-3">
    <div class="col-sm-12" id="fig1">
        <figure>
            <img src="/2026/assets/img/2026-04-27-content-promotion-agent-design/banana-search.jpg" class="img-fluid rounded z-depth-1" alt="Content optimization in a traditional search setting">
            <figcaption class="caption" style="color: inherit;">
                <strong>Figure 1:</strong> Content optimization in a traditional search setting.
            </figcaption>
        </figure>
    </div>
</div>

<div class="row mt-3">
    <div class="col-sm-12" id="fig2">
        <figure>
            <img src="/2026/assets/img/2026-04-27-content-promotion-agent-design/banana-chat-fixed-with-gpt.png" class="img-fluid rounded z-depth-1" alt="Content optimization in a chat-like setting">
            <figcaption class="caption" style="color: inherit;">
                <strong>Figure 2:</strong> Content optimization in a chat-like setting.
            </figcaption>
        </figure>
    </div>
</div>

## Content Promotion as a Game

We argue that game theory provides a major and essential framework for analyzing the
resulting competitive dynamics and for designing agents that assist publishers to
strategically promote their content. More precisely, repeated games with incomplete
information <d-cite key=aumann_repeated_1995></d-cite>, induced by the system (search or chat-like system). Actions correspond to content generation (or modification). Strategies are
history dependent content selection/generation algorithms and strategy profiles capture joint
activity of agents which are active in the ecosystem. The utility function of an agent is the
accumulated payoff along time, taking into account aspects such as ranking (for traditional
search), visibility (for the chat-like systems) and faithfulness to the original content before
any modification. As the selection/generation mechanism is only partially known, the setting
is of incomplete information. Since the publisher may keep improving the content once she
receives feedback from the system, we argue that the game should be treated as a repeated
game. Figures [3](#fig3) and [4](#fig4) present schematic illustrations of the resulting games for search
systems and chat-like systems, respectively.

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0" id="fig3">
        <figure>
            <img src="/2026/assets/img/2026-04-27-content-promotion-agent-design/search-schema-nbg.png" class="img-fluid rounded z-depth-1" alt="Schematic illustration of a traditional search game">
            <figcaption class="caption text-center" style="color: inherit;">
                <strong>Figure 3:</strong> Schematic illustration of a traditional search game.
            </figcaption>
        </figure>
    </div>

    <div class="col-sm mt-3 mt-md-0" id="fig4">
        <figure>
            <img src="/2026/assets/img/2026-04-27-content-promotion-agent-design/chat-schema-nbg.png" class="img-fluid rounded z-depth-1" alt="Schematic illustration of a chat-like game">
            <figcaption class="caption text-center" style="color: inherit;">
                <strong>Figure 4:</strong> Schematic illustration of a chat-like game.
            </figcaption>
        </figure>
    </div>
</div>

The viewpoint just presented is aligned with a growing body of research that treats content
promotion as a strategic process. During recent years, several works have applied game-theoretic approaches to model competitive search <d-cite key=kurland_competitive_2022></d-cite>,
diversity-based ranking <d-cite key=mordo_ameliorating_2025></d-cite>, to study the convergence of long
term behavior to equilibrium in retrieval settings <d-cite key="madmon_search_2024, madmon_convergence_2024"></d-cite> and to design
equilibrium in ranking games using corpus enrichment <d-cite key=nachimovsky_power_2025></d-cite>. <d-cite key=ye_llm_empowered_2025></d-cite> explored strategic document modifications in response to ranking
feedback, revealing rich and sometimes counterintuitive dynamics. Yao et al.’s work provides
deep, game-theoretic insight into how creators behave strategically in recommender
systems and offers actionable designs for systems to align creator incentives with user
welfare <d-cite key="yao_how_2023, yao_user_2024, yao_human_2024"></d-cite>. In the context of
strategic retrieval systems, <d-cite key=mordo_sponsored_2024></d-cite> presented the first
formal game-theoretic analysis of a sponsored QA system where advertisers bid to have
their sponsored content fused in the generated answer of question-answering systems. <d-cite key=mordo_sponsored_2024></d-cite> showed how this new ad-integration model
introduces strategic bidding behavior and derived equilibrium properties and welfare
implications in this setting.

However, the technological transformation of the last few years --- particularly the rise of
LLM-based chat interfaces --- calls for re-examination and adaptation of the assumptions in
classical game theoretic models. Traditional work often assumes fixed ranking functions and
explicit publishers’ incentives. In contrast, chat-like systems generate personalized and
dynamic answers <d-cite key=metzler_rethinking_2021></d-cite>; the objective of publishers becomes much harder to
define. These shifts motivate the development of new models that account for generative
selection mechanisms, opaque feedback, and high adaptivity --- all of which strengthen the
case for game-theoretic reasoning and for new solution concepts.

## Agent Design Aspects for Traditional Search and Chat-like Systems

We delve into three important aspects that should be considered when designing agents that
assist publishers promote their content.

The first ingredient that can (and will) be naturally used, is to exploit the significant power of
LLMs, e.g., use clever prompt engineering to promote content and maintain faithfulness in
such games.

A second ingredient one should be aware of is incomplete information about the selection or
generation mechanism applied by the system. Using, for example, information about the decision made by the mechanism (e.g., past responses) can help to train and fine tune
agents.

The third ingredient is the need to be aware of competitors’ behavior. With limited access to
annotated data --- and only partial observability of the decisions made by these systems ---
this calls for carefully designed simulation tools. here again LLMs can be used as effective
generative tools.

Heretofore, we have described content promotion as a publishers game, shaped by evolving
search and chat-based ecosystems and by the increasing role of generative AI. We now
discuss different frameworks that can be employed to model and design effective agents in
such settings.

## Frameworks
### Multi-Agent Reinforcement Learning (MARL)
This framework models each player as an autonomous learning agent that optimizes its
policy through repeated interaction with the environment and with other agents. MARL
enables us to study how strategic behaviors emerge end-to-end from experience, without
imposing strong assumptions about how these systems (i.e., search or chat-like systems)
operate internally <d-cite key="gemp_developing_2022, guo_large_2024, shoham_if_2025, zhou_multi_agent_2025"></d-cite>. It also allows us to analyze coordination, competition, and non-
stationarity --- core properties of competitive games under a principled RL formulation. In our
setting, the policy of the agent is the LLM itself, and reinforcement learning is the mechanism
that aligns this LLM to the objectives of the game, given whatever information the agent can
observe; different RL alignment algorithms may be adopted; e.g., <d-cite key="schulman_proximal_2017, rafailov_direct_2024, calandriello_multi-turn_2024, zhou_wpo_2024, shao_deepseekmath_2024"></d-cite>. If we
had access to a full log of games --- for example, historical queries and the ranking of pages
in traditional search, or user questions and the resulting responses in chat-like systems ---
then we could directly formulate a supervised or RL-based learning problem. With such logs,
the agent could be trained to optimize performance with respect to well-defined metrics (e.g.,
rank position, click-through, user engagement, rejection rates, etc.). Beyond the standard
trade-offs encountered in reinforcement learning, our settings introduce unique design
tensions that must be addressed:

#### (a) Short-term wins vs. long-term information leakage
An agent may be able to win an individual round of a game by aggressively optimizing its
content. However, doing so may reveal its strategy to competing agents, enabling them to
quickly imitate <d-cite key=raifer_information_2017></d-cite> or counter it. Since the objective is to maximize cumulative
reward across many rounds, the agent must balance immediate gains with strategic opacity ---
sometimes deliberately avoiding overly distinctive or extreme actions to prevent being copied <d-cite key=aumann_repeated_1995></d-cite>.

#### (b) On-policy vs. off-policy training
A crucial design question is whether the agent should continue learning during the game
(on-policy) or rely solely on offline training before the game begins (off-policy).
- **Online adaptation** allows the agent to track shifting competitor behaviors or evolving
ranking/generation mechanisms.
- **Offline training** offers stability and avoids overfitting to short-term noise or
adversarial manipulations.
Choosing between online and offline modes --- or combining them --- depends on the
environment’s stability, observability, and the computational budget during gameplay.

#### (c) Constructing the loss function
A core challenge is designing a loss function that balances ranking promotion or content
visibility with faithfulness to the original content. On one hand, the agent should modify
content to improve performance; on the other hand, excessive deviation risks harming
credibility, violating constraints, or triggering penalties. The loss function must encode this
balance explicitly --- penalizing harmful drift while incentivizing strategic, beneficial content
modifications.

### Synthetic Dataset Generation
Sometimes real-world data presents substantial limitations: it may be inaccessible, noisy,
incomplete, or simply ill-suited for the specific questions we wish to study. For this reason,
we propose constructing synthetic datasets with controlled statistical properties. Synthetic
data enables us to systematically probe targeted hypotheses --- for example, how diversity
constraints, noise structure, or shifts in the query or question distribution influence agent
behavior and system-level outcomes. Moreover, synthetic datasets provide repeatability and
experimental control that are often impossible with logs of production systems. They allow
for clean ablation studies, controlled perturbations, and the isolation of causal factors, all of
which are essential for understanding and improving agents operating in competitive ranking
and chat-like ecosystems. Importantly LLMs now make it feasible to generate high-fidelity
synthetic datasets at scale, enabling richer and more flexible experimental environments
than were previously possible.

### Simulation Environment
A simulation environment that models the rules of the game, the ranking or generation
function, and the interaction protocol between agents is essential in the discussed strategic
ecosystems <d-cite key=kovarik_game_2024></d-cite>. Such simulation can be used for
generating strategic synthetic training datasets and for evaluating agents under diverse
conditions. Together with the increasing proliferation over the Web of generated AI content <d-cite key=spennemann_delving_2025></d-cite>, we suggest using simulators of the discussed competitions. Such
simulators support rapid iteration and controlled experimentation across a wide range of
configurations --- rankers, prompts, datasets, reward structures, and hyperparameters. By
isolating specific components of the system, the simulation environment helps uncover
causal relationships, identify sensitivities to design choices, and systematically analyze how
agents behave under different competitive scenarios.

### Self-Play
Self-play is a method for iteratively
training agents to be improved by competing (via simulation) against evolving versions of
themselves <d-cite key="fang_serl_2025, fu_improving_2023, zhang_survey_2025"></d-cite>. This approach is particularly useful in settings where optimal strategies are not
known a priori and where the competitive landscape shifts over time. Self-play helps uncover
equilibrium behaviors, stability properties, and potential failure modes of learning algorithms
in adversarial or co-adaptive settings.

### Meta Games and Learning Equilibrium
Agent design itself introduces an additional strategic layer: designers may choose between
different LLM architectures, training procedures, datasets, prompting strategies, or RL
frameworks. In this broader perspective, agents are not only competing within a single
ranking or generation game, but the designers are competing in a meta-game over
agent designs <d-cite key="li_meta_game_2024, nachimovsky_multi_agent_2025, shapira_glee_2025"></d-cite>. Long-run success therefore depends on performing well not only within the
fixed game, but also across the distribution of possible design choices made by other
participants. In such meta-games, no single deterministic design may be universally optimal.

Instead, probabilistic ensembles of designs --- mixtures over algorithms, prompting strategies,
or training regimes --- may provide better robustness and theoretical guarantees. These
ensemble-based strategies reduce exploitability, adapt better to heterogeneous opponents,
and align with classical results in game theory showing that mixed strategies often
outperform pure ones in competitive environments.

Within meta-games over agent designs, a natural solution concept is learning equilibrium <d-cite key=monderer_learning_2007></d-cite>. Rather than focusing on equilibria of the underlying
content-generation game --- where actions correspond to the content produced --- learning
equilibrium concerns the design algorithms themselves being in equilibrium. That is, each
designer’s strategy for constructing agents is optimal given the strategies of other designers,
forming a stable configuration at the meta-level.

This distinction is subtle but important: learning equilibrium ensures that agent designs
collectively converge to a stable distribution, even as the content-generation game continues
to evolve. Such a concept becomes particularly relevant in scenarios where a platform or
cloud provider offers content-generation services to multiple competing parties. In this
setting, providing agent designs that satisfy learning equilibrium naturally mitigates
exploitability, promotes fairness, and guarantees robust performance across heterogeneous
competitors.

## Preliminary Results
We now turn to discuss several papers that illustrate components (or the absence thereof) of
the frameworks discussed above.

### Prompt-based Agent
&nbsp;<d-cite key=bardas_automatic_2025></d-cite> introduced a set of prompt-based agents designed to modify a human-authored
document so that the updated version is ranked higher in the next round of the ranking. Their
prompting strategy explicitly constrains the agent to produce revisions that remain faithful ---
at least to some extent --- to the original document. <d-cite key=bardas_automatic_2025></d-cite> introduced prompt-
engineering techniques that, in an offline evaluation, outperformed human-authored
documents without requiring any learning. Their evaluation was conducted using data from a
ranking competition previously organized among human document authors.

### RLRF-based Agent
&nbsp;<d-cite key="mordo_rlrf_2025"></d-cite> proposed a ranker-aligned agent, trained via a reinforcement-
learning-from-ranker-feedback (RLRF) framework. They constructed a synthetic dataset
generated through a simulated ranking competition and used it to train the agent with the
DPO algorithm <d-cite key=rafailov_direct_2024></d-cite>. Notably, the resulting agent learned to adapt both to
the ranking mechanism and to the behavior of competing agents. Their simulation-based
evaluation demonstrates robustness with respect to various environmental conditions,
including the number of competitors and the choice of ranking function. The simulation
assumed that the competing agents are nonoptimized agents; this setup can be viewed as a
variant of self-play, where agents iteratively interact under stable, nonlearning strategies.

### LEMSS
&nbsp;<d-cite key="mordo_lemss_2025" block></d-cite> also introduced LEMSS, a large-scale simulation framework
for ranking competitions among LLM-based agents. Their results show that LLM-driven
agents exhibit behavioral patterns like those of humans in iterative ranking games. LEMSS
provides fine-grained control over nearly all aspects of the competition --- such as the number
of competitors, the ranking mechanism, and the reward structure --- and enables
comprehensive analysis of the induced repeated game. The simulation models a traditional ranking competition. To design agents capable of promoting content in a chat-like interaction
setting, a simulation of such a mechanism is called for.

### Attacking RAG-based systems
Recent work highlights the growing vulnerability of retrieval-augmented generation (RAG)
systems to targeted manipulation. PoisonedRAG <d-cite key=zou_poisonedrag_2024></d-cite> shows that inserting only
a few malicious documents into a RAG system’s knowledge base can steer model outputs
with high success, revealing a potent “knowledge-corruption” attack surface. In contrast,
RIPRAG <d-cite key=xi_riprag_2025></d-cite> use reinforcement learning to craft effective poisoned documents
without access to the system’s internals. <d-cite key=wu_what_2025></d-cite> suggests to cooperatively optimize
content for generative search engines by learning their preference rules. However, all these
papers overlook the competitive nature of real-world environments, where multiple strategic
content creators adaptively improve their content --- potentially using agents --- in a repeated
game. This missing game-theoretic perspective limits their ability to capture the dynamics
and equilibria that emerge when many actors simultaneously attempt to influence RAG-
based systems.

## Tools
Designing agents for ranking and content promotion competition benefits from a growing
ecosystem of LLM-oriented development tools. Copilot Studio, Vertex AI, and LangChain
provide the infrastructure needed to orchestrate complex LLM workflows, connect models to
external data, and manage iterative agent-environment interactions. Copilot Studio enables
rapid prototyping of agent behaviors within controlled interfaces, while Vertex AI offers
scalable model hosting, fine-tuning, and evaluation pipelines suited for experimentation at
scale. LangChain adds modular abstractions --- such as memory, tools, retrieval, and multi-
step reasoning --- that make it easier to construct agents capable of interacting with dynamic
environments. In addition, there are many python packages that may help to design and
simulate RL-based agents such as <d-cite key="feng_agile_2024, liu_agentlite_2024"></d-cite>.

## Conclusion
In this blog post, we discussed how LLMs are reshaping the search ecosystem --- both in
traditional search and in emerging chat-like question-answering systems. Beyond
transforming how users search, LLMs also change how publishers operate, enabling them to
leverage LLMs to strategically optimize and promote iteratively their content.

We discussed several frameworks for modelling and designing such agents, including game-
theoretic models and reinforcement learning approaches. These frameworks help account
for multiple layers of uncertainty: the opaque ranking model or response-generation
mechanisms of retrieval systems, the dynamic nature of evolving user queries/questions,
and the competitive behavior of other strategic publishers or human content creators.

To build robust and adaptable agents, we emphasized that simulation plays a crucial role.
Simulations enable the creation of strategic synthetic datasets and provide controlled
environments for evaluating agent behavior, stress-testing policy robustness, and studying
long-term dynamics in repeated competitive scenarios.

Overall, as the ecosystem continues to evolve, combining insights from game theory,
reinforcement learning, and simulation offers a principled path toward designing agents
capable of succeeding in both current and future search paradigms.